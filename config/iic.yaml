optimizer:
    lr: 2.5e-4
    eps: 1e-5
    clip_grad: .5

embedding:
    size: 16
    history_size: 50
    hidden_size: 64
    epochs: 10
    batch_size: 512
    n_step: 3
    lr: 5e-4
    pretrain: .15

agent:
    pi_clip: .1
    gamma: .99
    epochs: 3
    batch_size: 256
    ent_k: .01
    val_loss_k: 1
    gae_lambda: .95

train:
    steps: 9766 # 10M / (128 * 8)
    rollout_size: 128
    num_env: 8
    log_every: 1
    checkpoint_every: 10000
    checkpoint_name: models/iic_{n_iter}.pt
